{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = 'v1.02-train'\n",
    "DATASET_ROOT = '/home/ezhang/lyft/v1.02-train/v1.02-train'\n",
    "ARTIFACTS_FOLDER = \"/home/ezhang/lyft/artifacts-2\"\n",
    "\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level5data = LyftDataset(json_path=DATASET_ROOT + \"/v1.02-train\", data_path=DATASET_ROOT)\n",
    "os.makedirs(ARTIFACTS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [record for record in level5data.scene]\n",
    "\n",
    "entries = []\n",
    "\n",
    "for record in records:\n",
    "\n",
    "    token = record['token']\n",
    "    name = record['name']\n",
    "    host = \"-\".join(record['name'].split(\"-\")[:2])\n",
    "    first_sample_token = record[\"first_sample_token\"]\n",
    "\n",
    "    entries.append((host, name, token, first_sample_token))\n",
    "            \n",
    "df = pd.DataFrame(entries, columns=[\"host\", \"scene_name\", \"scene_token\", \"first_sample_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives ~ 80/20 Train Validation Split\n",
    "\n",
    "validation_hosts = [\"host-a007\", \"host-a008\"]\n",
    "\n",
    "validation_df = df[df[\"host\"].isin(validation_hosts)]\n",
    "vi = validation_df.index\n",
    "train_df = df[~df.index.isin(vi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing. Most of these are taken from lyft's dataset tutorials.\n",
    "\n",
    "def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n",
    "    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n",
    "    \n",
    "    tm = np.eye(4, dtype=np.float32)\n",
    "    translation = shape/2 + offset/voxel_size\n",
    "    \n",
    "    tm = tm * np.array(np.hstack((1/voxel_size, [1])))\n",
    "\n",
    "    tm[:3, 3] = np.transpose(translation)\n",
    "    return tm\n",
    "\n",
    "def transform_points(points, transf_matrix):\n",
    "    if points.shape[0] not in [3,4]:\n",
    "        raise Exception(\"Points input should be (3,N) or (4,N) shape, received {}\".format(points.shape))\n",
    "    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n",
    "\n",
    "def car_to_voxel_coords(points, shape, voxel_size, z_offset=0):\n",
    "    if len(shape) != 3:\n",
    "        raise Exception(\"Voxel volume shape should be 3 dimensions (x,y,z)\")\n",
    "        \n",
    "    if len(points.shape) != 2 or points.shape[0] not in [3, 4]:\n",
    "        raise Exception(\"Input points should be (3,N) or (4,N) in shape, found {}\".format(points.shape))\n",
    "\n",
    "    tm = create_transformation_matrix_to_voxel_space(shape, voxel_size, (0, 0, z_offset))\n",
    "    p = transform_points(points, tm)\n",
    "    return p\n",
    "\n",
    "def create_voxel_pointcloud(points, shape, voxel_size, z_offset):\n",
    "\n",
    "    points_voxel_coords = car_to_voxel_coords(points.copy(), shape, voxel_size, z_offset)\n",
    "    points_voxel_coords = points_voxel_coords[:3].transpose(1,0)\n",
    "    points_voxel_coords = np.int0(points_voxel_coords)\n",
    "        \n",
    "    bev = np.zeros(shape, dtype=np.float32)\n",
    "    bev_shape = np.array(shape)\n",
    "\n",
    "    within_bounds = (np.all(points_voxel_coords >= 0, axis=1) * np.all(points_voxel_coords < bev_shape, axis=1))\n",
    "    \n",
    "    points_voxel_coords = points_voxel_coords[within_bounds]\n",
    "    coord, count = np.unique(points_voxel_coords, axis=0, return_counts=True)\n",
    "        \n",
    "    bev[coord[:,1], coord[:,0], coord[:,2]] = count\n",
    "    \n",
    "    return bev\n",
    "\n",
    "def normalize_voxel_intensities(bev, max_intensity=16):\n",
    "    return (bev/max_intensity).clip(0,1)\n",
    "\n",
    "def move_boxes_to_car_space(boxes, ego_pose):\n",
    "    translation = -np.array(ego_pose['translation'])\n",
    "    rotation = Quaternion(ego_pose['rotation']).inverse\n",
    "    for box in boxes:\n",
    "        box.translate(translation)\n",
    "        box.rotate(rotation)\n",
    "        \n",
    "def scale_boxes(boxes, factor):\n",
    "    for box in boxes:\n",
    "        box.wlh = box.wlh * factor\n",
    "\n",
    "def draw_boxes(im, voxel_size, boxes, z_offset=0.0):\n",
    "    for box in boxes:\n",
    "        corners = box.bottom_corners()\n",
    "        corners_voxel = car_to_voxel_coords(corners, im.shape, voxel_size, z_offset).transpose(1,0)\n",
    "        # Drop z coordinate (to get 2d)\n",
    "        corners_voxel = corners_voxel[:,:2]\n",
    "\n",
    "        # Objects will have class value 1\n",
    "        class_value = 1\n",
    "\n",
    "        cv2.drawContours(im, np.int0([corners_voxel]), 0, (class_value, class_value, class_value), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training/testing data\n",
    "\n",
    "voxel_size = (0.4, 0.4, 1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336, 336, 3)\n",
    "\n",
    "box_scale = 0.8\n",
    "\n",
    "train_data_folder = os.path.join(ARTIFACTS_FOLDER, \"./bev_train_data\")\n",
    "validation_data_folder = os.path.join(ARTIFACTS_FOLDER, \"./bev_validation_data\")\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def prepare_training_data_for_scene(first_sample_token, output_folder, bev_shape, voxel_size, z_offset, box_scale):\n",
    "    sample_token = first_sample_token\n",
    "    \n",
    "    while sample_token is not None:\n",
    "        \n",
    "        sample = level5data.get(\"sample\", sample_token)\n",
    "\n",
    "        sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "        lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "        lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "\n",
    "        ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "        calibrated_sensor = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "\n",
    "\n",
    "        global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                           Quaternion(ego_pose['rotation']), inverse=False)\n",
    "\n",
    "        car_from_sensor = transform_matrix(calibrated_sensor['translation'], Quaternion(calibrated_sensor['rotation']),\n",
    "                                            inverse=False)\n",
    "\n",
    "        lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "        lidar_pointcloud.transform(car_from_sensor)\n",
    "        \n",
    "        bev = create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "        bev = normalize_voxel_intensities(bev)\n",
    "\n",
    "        boxes = level5data.get_boxes(sample_lidar_token)\n",
    "\n",
    "        target = np.zeros_like(bev)\n",
    "\n",
    "        move_boxes_to_car_space(boxes, ego_pose)\n",
    "        scale_boxes(boxes, box_scale)\n",
    "        draw_boxes(target, voxel_size, boxes=boxes, z_offset=z_offset)\n",
    "\n",
    "        bev_im = np.round(bev*255).astype(np.uint8)\n",
    "        target_im = target[:,:,0] # 3-channel -> 1-channel (binary) image\n",
    "\n",
    "        cv2.imwrite(os.path.join(output_folder, \"{}_input.png\".format(sample_token)), bev_im)\n",
    "        cv2.imwrite(os.path.join(output_folder, \"{}_target.png\".format(sample_token)), target_im)\n",
    "        \n",
    "        # next frame in scene\n",
    "        sample_token = sample[\"next\"]\n",
    "\n",
    "for df, data_folder in [(train_df, train_data_folder), (validation_df, validation_data_folder)]:\n",
    "    print(\"Preparing data into {} using {} workers\".format(data_folder, NUM_WORKERS))\n",
    "    first_samples = df.first_sample_token.values\n",
    "\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "    \n",
    "    process_func = partial(prepare_training_data_for_scene,\n",
    "                           output_folder=data_folder, bev_shape=bev_shape, voxel_size=voxel_size, z_offset=z_offset, box_scale=box_scale)\n",
    "\n",
    "    pool = Pool(NUM_WORKERS)\n",
    "    for _ in tqdm_notebook(pool.imap_unordered(process_func, first_samples), total=len(first_samples)):\n",
    "        pass\n",
    "    pool.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
